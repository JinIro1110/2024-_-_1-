{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bbb9MJZI5GS",
        "outputId": "b55e0da1-471e-4467-c72a-361fe042bd12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.31.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from transformers import AutoTokenizer, PreTrainedTokenizerFast, GPT2LMHeadModel, Trainer, TrainingArguments, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhzI8_TOI7_Y",
        "outputId": "2a27e87e-b5f2-4788-dfb3-2e9cb0be57ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tale.txt 파일 읽기\n",
        "txt_file_path = \"/content/drive/MyDrive/Tale/tale.txt\"\n",
        "with open(txt_file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# 일부분 출력\n",
        "print(text.split('\\n')[:5])\n",
        "\n",
        "print(len(text.split('\\n\\n\\n')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tPYOC5RI6nj",
        "outputId": "9033d360-affa-4c68-8fdb-c3b9f1340202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['늑대가 양떼무리에서 떨어진 어린양을 만났어요.', '그래서 잡아먹기로 작정했지요.', '그래서 그렇듯 한 이야기를 하며 잡아먹을 생각을 했어요.', '늑대가 어린양에게 말했어요. \"어이, 네가 작년에 나 욕했지.\"', '\"진짜 아니에요...\" 라며 어린양이 애처로운 목소리로 말했어요. \"전 작년에 태어나지도 않은걸요…\"']\n",
            "202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "  pad_token='<pad>', mask_token='<mask>')\n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgwkOnVxJEyM",
        "outputId": "99361275-1c88-40cc-b041-4ed69e3c2222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from transformers import AutoTokenizer, PreTrainedTokenizerFast, Trainer, TrainingArguments, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "txt_file_path = \"/content/drive/MyDrive/Tale/tale.txt\"\n",
        "with open(txt_file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# 동화들로 분리 (\\n\\n\\n 사용)\n",
        "fairy_tales = text.split('\\n\\n\\n')\n",
        "print(f\"총 {len(fairy_tales)}개의 동화를 읽었습니다.\")\n",
        "\n",
        "# 토크나이저 및 모델 초기화\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "  pad_token='<pad>', mask_token='<mask>')\n",
        "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# 데이터셋 분할 (학습 80%, 검증 20%)\n",
        "np.random.seed(42)\n",
        "indices = np.random.permutation(len(fairy_tales))\n",
        "train_indices = indices[:int(0.8*len(indices))]  # 변경: train_idx -> train_indices\n",
        "val_indices = indices[int(0.8*len(indices)):]  # 변경: val_idx -> val_indices\n",
        "train_tales = [fairy_tales[i] for i in train_indices]\n",
        "val_tales = [fairy_tales[i] for i in val_indices]\n",
        "\n",
        "# 동화 데이터셋 클래스\n",
        "class FairyTaleDataset(Dataset):\n",
        "    def __init__(self, fairy_tales, tokenizer, max_length=1024):\n",
        "        self.fairy_tales = fairy_tales\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fairy_tales)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tale = self.fairy_tales[idx]\n",
        "        encoding = self.tokenizer(tale, truncation=True, max_length=self.max_length)\n",
        "        return encoding\n",
        "\n",
        "# 동적 배치 크기를 위한 데이터 콜레이터\n",
        "class DynamicDataCollatorForLanguageModeling(DataCollatorForLanguageModeling):\n",
        "    def __call__(self, examples):\n",
        "        # 길이에 따라 정렬\n",
        "        examples = sorted(examples, key=lambda x: len(x[\"input_ids\"]), reverse=True)\n",
        "        batch = super().__call__(examples)\n",
        "        batch[\"labels\"][batch[\"input_ids\"] == self.tokenizer.pad_token_id] = -100\n",
        "        return batch\n",
        "\n",
        "# 데이터셋 및 데이터 콜레이터 설정\n",
        "train_dataset = FairyTaleDataset(train_tales, tokenizer)\n",
        "val_dataset = FairyTaleDataset(val_tales, tokenizer)\n",
        "data_collator = DynamicDataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# 훈련 인자 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Tale/kogpt2_fairy_tales\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    logging_steps=100,\n",
        ")\n",
        "# Perplexity 계산 함수\n",
        "def compute_perplexity(model, dataset, data_collator, device):\n",
        "    model.eval()\n",
        "    dataloader = DataLoader(dataset, batch_size=8, collate_fn=data_collator, shuffle=False)\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            non_padded_tokens = torch.sum(attention_mask)\n",
        "            total_loss += loss.item() * non_padded_tokens.item()\n",
        "            total_tokens += non_padded_tokens.item()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
        "    return perplexity\n",
        "\n",
        "# Trainer 설정 및 학습\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tuning 전 Perplexity 계산\n",
        "pre_train_perplexity = compute_perplexity(model, val_dataset, data_collator, device)\n",
        "print(f\"Fine-tuning 전 Perplexity: {pre_train_perplexity:.2f}\")\n",
        "\n",
        "# 모델 학습\n",
        "trainer.train()\n",
        "\n",
        "# Fine-tuning 후 Perplexity 계산\n",
        "post_train_perplexity = compute_perplexity(model, val_dataset, data_collator, device)\n",
        "print(f\"Fine-tuning 후 Perplexity: {post_train_perplexity:.2f}\")\n",
        "\n",
        "# 최종 모델 저장\n",
        "trainer.save_model(\"/content/drive/MyDrive/Tale/fine_tuned_kogpt2_fairy_tales\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "gQNoDdw_VLTK",
        "outputId": "a2e64a93-53ef-4435-fa20-0e12bdf373af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "총 202개의 동화를 읽었습니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning 전 Perplexity: 26.95\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [210/210 2:10:39, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.195273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.151115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.143563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.176833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.221823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.243867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.282624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.293279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.309012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.314138</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning 후 Perplexity: 28.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast, AutoModelForCausalLM\n",
        "\n",
        "# 토크나이저 및 모델 불러오기\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "  pad_token='<pad>', mask_token='<mask>')\n",
        "\n",
        "model_path = '/content/drive/MyDrive/Tale/fine_tuned_kogpt2_fairy_tales'  # 미세조정된 모델 경로\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "model.eval()  # 평가 모드로 설정\n",
        "\n",
        "# 텍스트 생성 함수\n",
        "def generate_fairy_tale(model, tokenizer, prompts, max_length=120, temperature=0.8, top_p=0.5, num_return_sequences=1):\n",
        "    tale = \"\"\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        if i > 0:\n",
        "            tale += \". \"\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            no_repeat_ngram_size=2,\n",
        "            do_sample=True\n",
        "        )\n",
        "        generated_chapter = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        tale += generated_chapter.capitalize() + \" 어요.\"\n",
        "\n",
        "    return tale\n",
        "\n",
        "# 고래 백경이 등장하는 각 챕터의 개요를 상상하여 미리 입력하기\n",
        "chapter_prompts = [\n",
        "    \"한 바다에 작은 어부가 살고 있었어요. 그의 이름은 영식이었어요. 어느 날, 영식이는 바다에 나가서 고래를 잡기로 결심했어요.\",\n",
        "    \"영식이는 고래를 만나게 되었어요. 그러나 그 고래는 사실 고래 백경이었어요. 고래 백경은 영식이에게 부탁을 했어요.\",\n",
        "    \"영식이는 고래 백경의 부탁을 들어주었어요. 그러면서 고래 백경은 영식이와 함께 모험을 떠났어요.\",\n",
        "    \"마지막으로, 영식이와 고래 백경은 함께 어려움을 극복하고 친구가 되었어요. 그리고 그들은 항상 서로를 기억하고 지켜줄 것을 약속했어요.\"\n",
        "]\n",
        "\n",
        "# 동화 생성 및 출력\n",
        "generated_tale = generate_fairy_tale(model, tokenizer, chapter_prompts)\n",
        "print(generated_tale)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrq3JGV0uuDK",
        "outputId": "bf4e8a76-3734-4714-86b2-04cae99c9235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한 바다에 작은 어부가 살고 있었어요. 그의 이름은 영식이었어요. 어느 날, 영식이는 바다에 나가서 고래를 잡기로 결심했어요. 고래는 그 해변을 어슬렁거리며 헤엄치고 있었어요.\n",
            "어느 날, 어부는 고래가 자신을 쫓고 있다는 것을 알게 되었어요.\n",
            "그때, 어부의 등에 타고 있던 작은 고래 한 마리가 갑자기 몸이 떨리고 있다는 걸 알았어요.\n",
            "그래서 어부를 고래로 착각하고서 바닷속으로 데리고 갔어요.\n",
            "고래는 고래에게 말했어요.\n",
            "\"저쪽에 큰 고래와 작은 고양이가 있을 거야. 그 고래들을 이용해서 그들을 어요.. 영식이는 고래를 만나게 되었어요. 그러나 그 고래는 사실 고래 백경이었어요. 고래 백경은 영식이에게 부탁을 했어요. \"제가 도와드리겠어요!\"\n",
            "영식이가 대답했어요.\n",
            "\"네, 부탁합니다.\"\n",
            "그런데 영식은 고래가 너무 고마워서 울음을 뚝 그쳤어요.\n",
            "\"네가 부탁하는 게 뭐가 있니?\"\n",
            "\"물론이죠. 제 부탁이 제게는 최고의 선물이에요.\"\n",
            "고래는 고래에게 감사의 인사를 전했어요.\n",
            "그리고 고래와 영식의 결혼을 축하했어요\n",
            "그들은 함께 행복하게 살았어요.\n",
            "하지만 어요.. 영식이는 고래 백경의 부탁을 들어주었어요. 그러면서 고래 백경은 영식이와 함께 모험을 떠났어요. 둘은 함께 바다 깊은 곳으로 모험을 떠나고 있었죠.\n",
            "그들은 서로 다른 점을 존중하고, 다른 이들을 존중하며 함께 즐거운 모험을 즐겼어요.\n",
            "어느 날, 고래는 자신의 몸에 불을 붙여 백경을 구했어요.\n",
            "그 바람에 백경이 숨을 쉬지 못했어요.\n",
            "백경은 고래를 구하기 위해 용감하게 나섰어요.\n",
            "그는 바다 속에서 고래와 만났어요.\n",
            "\"안녕, 백경아! 함께해줘서 고마워요. 함께라면 어떤 어려움도 극복할 수 있을 거에요.\"\n",
            "백경이 어요.. 마지막으로, 영식이와 고래 백경은 함께 어려움을 극복하고 친구가 되었어요. 그리고 그들은 항상 서로를 기억하고 지켜줄 것을 약속했어요. \n",
            "\"안녕, 백경아! 너의 모험은 정말 즐거웠어!\"\n",
            "백경은 자신의 모험이 끝난 후 활짝 웃었어요.\n",
            "\"이렇게 즐거우면 너에게 많은 것을 줄게.\"\n",
            "고래는 웃으며 대답했어요.\n",
            "\"맞아, 정말이었어. 너와 함께 모험을 떠나기로 했으니까.\"\n",
            "백경과 고래는 함께한 모험을 마치고, 바닷속을 탐험하기 시작했어요.\n",
            "그들은 넓은 들판에서 다양한 물고기들을 만났어요.\n",
            "그 중 어요.\n"
          ]
        }
      ]
    }
  ]
}